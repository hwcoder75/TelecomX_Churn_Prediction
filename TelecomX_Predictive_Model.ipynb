{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNd3zQ3Xd9WQjGHlkPbmKz+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JAohlJUCwLn-"},"outputs":[],"source":["# Telecom X - Segunda Parte del Challenge (Predicci√≥n de Churn)\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n","from xgboost import XGBClassifier\n","\n","sns.set(style=\"whitegrid\")\n","plt.rcParams[\"figure.figsize\"] = (10, 6)\n","\n","# === 1. Carga de datos tratados ===\n","df = pd.read_csv(\"TelecomX_Data_Treated.csv\")\n","\n","# === 2. An√°lisis exploratorio adicional ===\n","print(\"Vista general del DataFrame:\")\n","print(df.head())\n","print(\"\\nResumen de columnas:\")\n","print(df.info())\n","\n","# Distribuci√≥n de la variable objetivo\n","print(\"\\nDistribuci√≥n de Churn:\")\n","print(df['Churn'].value_counts(normalize=True))\n","sns.countplot(data=df, x='Churn')\n","plt.title(\"Distribuci√≥n de Evasi√≥n (Churn)\")\n","plt.show()\n","\n","# === 3. Preparaci√≥n de variables ===\n","df = df.drop(columns=[\"CustomerID\"], errors='ignore')\n","\n","# Codificaci√≥n de variables categ√≥ricas\n","cat_cols = df.select_dtypes(include='object').columns\n","df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n","\n","# Separar X e y\n","X = df.drop(\"Churn\", axis=1)\n","y = df[\"Churn\"]\n","\n","# Normalizar\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Divisi√≥n\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# === 4. Modelado ===\n","log_model = LogisticRegression()\n","log_model.fit(X_train, y_train)\n","y_pred_log = log_model.predict(X_test)\n","\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","y_pred_rf = rf_model.predict(X_test)\n","\n","xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","xgb_model.fit(X_train, y_train)\n","y_pred_xgb = xgb_model.predict(X_test)\n","\n","# === 5. Evaluaci√≥n ===\n","print(\"\\n--- Logistic Regression ---\")\n","print(classification_report(y_test, y_pred_log))\n","\n","print(\"\\n--- Random Forest ---\")\n","print(classification_report(y_test, y_pred_rf))\n","\n","print(\"\\n--- XGBoost ---\")\n","print(classification_report(y_test, y_pred_xgb))\n","\n","# ROC Curve\n","models = {\n","    \"Logistic\": log_model,\n","    \"Random Forest\": rf_model,\n","    \"XGBoost\": xgb_model\n","}\n","\n","plt.figure(figsize=(10,6))\n","for name, model in models.items():\n","    y_prob = model.predict_proba(X_test)[:,1]\n","    fpr, tpr, _ = roc_curve(y_test, y_prob)\n","    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc_score(y_test, y_prob):.2f})\")\n","plt.plot([0,1],[0,1],'k--')\n","plt.title(\"Curvas ROC\")\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","# === 6. Importancia de variables ===\n","importances = rf_model.feature_importances_\n","feat_names = X.columns\n","feat_imp = pd.DataFrame({'Feature': feat_names, 'Importance': importances})\n","feat_imp = feat_imp.sort_values(by='Importance', ascending=False).head(10)\n","\n","sns.barplot(data=feat_imp, y=\"Feature\", x=\"Importance\", palette=\"viridis\")\n","plt.title(\"Top 10 variables m√°s importantes (Random Forest)\")\n","plt.show()\n","\n","# === 7. Conclusi√≥n estrat√©gica ===\n","print(\"\"\"\n","üìä CONCLUSIONES ESTRAT√âGICAS:\n","\n","1. Las variables m√°s influyentes en la evasi√≥n de clientes fueron:\n","   - Tiempo de contrato, tipo de servicio, forma de pago, soporte t√©cnico, etc.\n","2. El modelo XGBoost present√≥ el mejor rendimiento en recall y AUC, lo que lo hace ideal para identificar clientes en riesgo.\n","3. Se recomienda a Telecom X:\n","   - Monitorear de cerca a los clientes con pagos mensuales altos y sin soporte t√©cnico.\n","   - Implementar ofertas personalizadas para quienes usan servicios b√°sicos o de contrato mes a mes.\n","   - Fortalecer canales de atenci√≥n al cliente para reducir cancelaciones por insatisfacci√≥n.\n","\n","‚úÖ Este modelo puede integrarse en un sistema de alertas para equipos de retenci√≥n y fidelizaci√≥n.\n","\"\"\")\n"]}]}